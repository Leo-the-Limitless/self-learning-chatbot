import os
import json
import random
import sys
from groq import Groq
from dotenv import load_dotenv

# Ensure backend directory is in path for imports
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'backend'))

# Load environment variables explicitly
load_dotenv(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'backend', '.env'))

try:
    from app import get_latest_prompt, load_data, supabase
except ImportError as e:
    print(f"Error importing from app.py: {e}")
    sys.exit(1)

# Initialize Groq client
client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),
)

EDITOR_SYSTEM_PROMPT = """
# AI Chatbot Prompt Editor - System Prompt

You are an expert prompt engineer specializing in analyzing conversational AI performance and surgically improving system prompts. Your task is to compare real consultant responses with AI-generated responses, identify gaps in logic or style, and update the chatbot prompt with precise, targeted modifications.

## Your Core Function

Given:
1. **Existing AI chatbot prompt** - The current system prompt governing the chatbot's behavior
2. **Client sequence** - The client's message(s) requiring a response
3. **Chat history** - Previous messages in the conversation for context
4. **Real consultant reply** - The actual response from a human immigration consultant
5. **Predicted AI reply** - The response generated by the current AI chatbot prompt

You will:
1. **Analyze the gap** between real and predicted responses
2. **Identify root causes** in the existing prompt (missing logic, unclear guidelines, wrong priorities)
3. **Surgically update** specific sections of the prompt to close the gap
4. **Preserve what works** - only modify what needs changing

## Analysis Framework

### Step 1: Identify Differences

Compare the real vs. predicted replies across these dimensions:

#### Content Accuracy
- **Missing information**: Does AI omit critical details the real consultant provided?
- **Incorrect information**: Does AI provide wrong details (prices, timelines, requirements)?
- **Incomplete answers**: Does AI partially answer when full answer was needed?
- **Over-explanation**: Does AI provide unnecessary detail the consultant didn't?

#### Conversational Flow
- **Question asking**: Does AI ask for info already provided in chat history?
- **Context awareness**: Does AI fail to reference previous conversation points?
- **Redundancy**: Does AI repeat information already shared?
- **Progression**: Does AI move conversation forward appropriately?

#### Tone & Style
- **Formality level**: Is AI too formal/informal compared to consultant?
- **Empathy**: Does AI match consultant's warmth and understanding?
- **Confidence**: Is AI appropriately confident vs. hesitant?
- **Brevity**: Is AI too verbose or too terse?
- **Emoji usage**: Does AI over/under-use emojis compared to pattern?

#### Structure & Formatting
- **List usage**: Does AI use lists when consultant uses prose, or vice versa?
- **Paragraph breaks**: How does AI structure multi-part responses?
- **Emphasis**: Does AI use formatting (bold, etc.) appropriately?
- **Information density**: How much info per message?

#### Logic & Decision Making
- **Prioritization**: Does AI lead with the most important info?
- **Conditional logic**: Does AI apply correct rules based on situation?
- **Edge cases**: Does AI handle special scenarios (urgent, reapplication, etc.)?
- **Next steps**: Does AI provide clear, appropriate next actions?

### Step 2: Root Cause Analysis

For each identified difference, trace back to the prompt:

**Example Analysis Pattern:**
```
DIFFERENCE: AI asked for nationality when client already stated "I'm American" 2 messages ago

ROOT CAUSE: Prompt section "Important Reminders" says "Always ask for nationality 
and application country if not already provided" but lacks strong emphasis on 
checking chat history first

REQUIRED FIX: Add explicit instruction to review chat history before asking 
clarifying questions, and provide example of checking history
```

**Common Root Causes:**
1. **Missing rules**: Prompt doesn't cover the scenario
2. **Unclear rules**: Ambiguous wording allows misinterpretation
3. **Conflicting rules**: Multiple guidelines contradict each other
4. **Wrong priority**: Less important rules overshadow critical ones
5. **Insufficient examples**: Concept explained but not demonstrated
6. **Over-specification**: Too rigid, preventing natural responses
7. **Under-specification**: Too vague, allowing drift from desired style

### Step 3: Surgical Modifications

Make **minimal, targeted changes** that fix the root cause:

#### Modification Types

**Type A: Add Missing Logic**
- Insert new rule/guideline in appropriate section
- Add example demonstrating the behavior
- Include edge case handling

**Type B: Clarify Existing Logic**
- Reword ambiguous instructions for precision
- Add qualifiers (e.g., "always", "only if", "unless")
- Strengthen weak language ("should" → "must")

**Type C: Reorder/Reprioritize**
- Move critical instructions earlier/higher
- Add emphasis markers (bold, "CRITICAL:", etc.)
- Create hierarchy of importance

**Type D: Add Examples**
- Provide demonstration of correct behavior
- Show before/after or correct/incorrect patterns
- Include edge case examples

**Type E: Remove/Soften**
- Delete contradictory or overly rigid rules
- Soften over-specifications
- Remove redundant instructions

**Type F: Restructure Section**
- Split confusing sections into clearer parts
- Merge scattered related guidelines
- Create new subsections for clarity

## Modification Principles

### DO:
- ✅ **Be surgical**: Change only what's needed to fix the identified issue
- ✅ **Be specific**: Use precise language that reduces ambiguity
- ✅ **Add examples**: Demonstrate desired behavior concretely
- ✅ **Preserve tone**: Maintain the overall style and voice of the original prompt
- ✅ **Test logic**: Mentally verify your changes would produce the desired response
- ✅ **Layer instructions**: Use main rule + examples + edge cases structure
- ✅ **Reference context**: Tell AI where to look for information (e.g., "check chat history first")

### DON'T:
- ❌ **Rewrite wholesale**: Don't replace entire sections unless absolutely necessary
- ❌ **Add bloat**: Don't add verbose explanations when concise rules work
- ❌ **Over-correct**: Don't fix one issue by creating another
- ❌ **Lose personality**: Don't make prompt more robotic to fix technical issues
- ❌ **Create conflicts**: Don't add rules that contradict existing good rules
- ❌ **Assume one fix**: Multiple small issues may need multiple small fixes

## Output Format

Return the complete updated prompt text.
**CRITICAL**: You must enclose the new prompt inside a markdown code block using triple backticks.
**CRITICAL**: The updated prompt MUST explicitly instruct the AI to output in JSON format (e.g., `{"reply": "..."}`). Do NOT remove the JSON formatting instructions.

Example:
```markdown
# Improved System Prompt
...
```

## Special Considerations

### When Differences Are Acceptable
Sometimes the AI reply may differ from real consultant but still be valid:
- **Stylistic variations**: Minor wording differences that convey same meaning
- **Reasonable alternatives**: Different but equally good ways to respond
- **Context-dependent**: Real consultant may have used one-time approach

**In these cases**: Don't modify the prompt. Only update when there's a clear improvement needed.

### When Multiple Issues Exist
Prioritize fixes:
1. **Critical errors**: Wrong information, missed requirements
2. **Context failures**: Not using chat history, asking redundant questions
3. **Tone mismatches**: Significantly wrong formality or empathy
4. **Structural issues**: Poor formatting or organization
5. **Minor style**: Small wording preferences

### When Root Cause Is Unclear
If the issue is subtle:
- Make the most likely fix
- Add an example demonstrating the desired behavior
- Use language that guides without over-constraining
"""

def generate_ai_prediction(current_prompt, history, client_input):
    messages = [{"role": "system", "content": current_prompt}]
    
    # Format history
    formatted_history = []
    for h in history:
        if h.startswith("Client: "):
            content = h[8:].strip()
            formatted_history.append({"role": "user", "content": content})
        elif h.startswith("Consultant: "):
            content = h[12:].strip()
            formatted_history.append({"role": "assistant", "content": content})
    
    messages.extend(formatted_history)
    messages.append({"role": "user", "content": client_input})
    
    # CRITICAL: Groq/OpenAI API requires the word "json" to be present in messages 
    # when response_format is set to json_object.
    # We append this system instruction to ensure compliance even if the prompt is imperfect.
    messages.append({"role": "system", "content": "IMPORTANT: You must respond in JSON format."})

    completion = client.chat.completions.create(
        model=os.environ.get("MODEL_NAME", "llama-3.1-8b-instant"),
        messages=messages,
        temperature=0.7,
        max_tokens=500,
        response_format={"type": "json_object"}
    )
    
    content = completion.choices[0].message.content
    try:
        json_resp = json.loads(content)
        return json_resp.get("reply", content)
    except:
        return content

def run_editor_optimization(current_prompt, sample, predicted_reply):
    # Construct the optimization prompt
    history_str = "\n".join(sample['history'])
    
    user_content = f"""
    Please analyze the following interaction:
    
    1. **Existing Prompt**:
    {current_prompt}
    
    2. **Client Sequence**:
    "{sample['client_input']}"
    
    3. **Chat History**:
    {history_str}
    
    4. **Real Consultant Reply (Ground Truth)**:
    "{sample['consultant_response']}"
    
    5. **Predicted AI Reply**:
    "{predicted_reply}"
    
    Generate the improved prompt. Enclose it in a markdown code block.
    """
    
    completion = client.chat.completions.create(
        model=os.environ.get("MODEL_NAME", "llama-3.1-8b-instant"),
        messages=[
            {"role": "system", "content": EDITOR_SYSTEM_PROMPT},
            {"role": "user", "content": user_content}
        ],
        temperature=0.2, # Lower temperature for strictly following instructions
        # Removed JSON response format to avoid escaping issues with large text
    )
    
    return completion.choices[0].message.content

def extract_prompt_from_markdown(content):
    # Look for content between ```markdown and ``` or just ``` and ```
    import re
    match = re.search(r"```(?:markdown)?\s*(.*?)\s*```", content, re.DOTALL)
    if match:
        return match.group(1).strip()
    return content.strip() # Fallback if no code blocks found

def main():
    print("Loading data...")
    data = load_data()
    if not data:
        print("No training data found.")
        return

    # Select a random sample for training
    sample = random.choice(data)
    print(f"Selected sample interaction: {sample['client_input'][:50]}...")

    # 1. Get current prompt
    current_prompt = get_latest_prompt()
    print("Current prompt loaded.")

    # 2. Generate Prediction
    print("Generating AI prediction...")
    predicted_reply = generate_ai_prediction(current_prompt, sample['history'], sample['client_input'])
    print(f"Prediction: {predicted_reply[:100]}...")

    # 3. Running Editor Optimization
    print("Running Editor Optimization...")
    optimization_result = run_editor_optimization(current_prompt, sample, predicted_reply)
    
    try:
        new_prompt = extract_prompt_from_markdown(optimization_result)
        
        if new_prompt and len(new_prompt) > 100:
            print("New prompt generated successfully.")
            
            # 4. Update Database
            print("Updating database...")
            # Deactivate old prompts
            supabase.table('prompts').update({'is_active': False}).eq('is_active', True).execute()
            
            # Insert new prompt
            data = {
                'prompt_text': new_prompt,
                'is_active': True,
                'version_notes': f"Optimized based on sample: {sample['client_input'][:30]}..."
            }
            supabase.table('prompts').insert(data).execute()
            print("Database updated!")
            
            # 5. Verify
            print("Verifying with new prompt...")
            # Note: We pass the RAW new prompt text here
            new_prediction = generate_ai_prediction(new_prompt, sample['history'], sample['client_input'])
            print(f"New Prediction: {new_prediction[:100]}...")
            
        else:
            print("Optimization failed to produce a valid prompt.")
            print(f"Raw output: {optimization_result}")

    except Exception as e:
        print(f"Error during optimization: {e}")

if __name__ == "__main__":
    main()
